{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":973292,"sourceType":"datasetVersion","datasetId":531544},{"sourceId":6011029,"sourceType":"datasetVersion","datasetId":2638370}],"dockerImageVersionId":30616,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### To do\n","metadata":{}},{"cell_type":"markdown","source":"> Two datasets \"MET-Meme & Memotion\" (Multimodal image and text) offensive Meme detection \n\n> \"Multiclass: Not offensive, slight offensive and very offensive\" \n\n> Two Models Models are parallelly work: \n    on text (BERT and CNN) on images (RESNet-152, also comment ResNet-50) \n    Then Dense layer, fully concatenation layer and final multi classification (softmax). \n    Requirement is Dataset should be 7k or more also balance dataset. \n    Accuracy, recall, f1score, precision (validation and training loss graph), \n    (validation and training accuracy graph), (Confusion Matrix)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"**Libraries**","metadata":{}},{"cell_type":"code","source":"rm -rf *pkl","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:30:57.814255Z","iopub.execute_input":"2024-05-14T20:30:57.814549Z","iopub.status.idle":"2024-05-14T20:30:58.798761Z","shell.execute_reply.started":"2024-05-14T20:30:57.814523Z","shell.execute_reply":"2024-05-14T20:30:58.797618Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nfrom PIL import Image\nimport cv2 \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport pickle\nimport os\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom transformers import BertTokenizer, BertModel\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torchvision.models import resnet152\nfrom transformers import BertModel, BertTokenizer","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:30:58.800344Z","iopub.execute_input":"2024-05-14T20:30:58.800741Z","iopub.status.idle":"2024-05-14T20:31:08.083238Z","shell.execute_reply.started":"2024-05-14T20:30:58.800702Z","shell.execute_reply":"2024-05-14T20:31:08.082282Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## EDA and Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### *Dataset Path*","metadata":{}},{"cell_type":"code","source":"ROOT_PATH = \"/kaggle/input/memotion-dataset-7k/memotion_dataset_7k\"\nROOT_PATH2 = \"/kaggle/input/met-meme\"","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:31:08.084485Z","iopub.execute_input":"2024-05-14T20:31:08.084878Z","iopub.status.idle":"2024-05-14T20:31:08.089096Z","shell.execute_reply.started":"2024-05-14T20:31:08.084853Z","shell.execute_reply":"2024-05-14T20:31:08.088134Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"data1 = pd.read_csv(f\"{ROOT_PATH}/labels.csv\")\n# data.drop(columns= [\"Unnamed: 0\", \"text_ocr\", \"humour\", \"sarcasm\", \"motivational\", \"overall_sentiment\"],\n#             inplace = True)\n# data.columns = [\"image_name\", \"text\", \"label\"]\n\n\ndata1.drop(columns= [\"Unnamed: 0\",\"text_ocr\", 'humour', 'sarcasm', 'motivational' , 'overall_sentiment'], inplace = True)\ndata1.columns = [\"image_name\", \"text\", \"label\"]\n\ndata1.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:31:08.092089Z","iopub.execute_input":"2024-05-14T20:31:08.092454Z","iopub.status.idle":"2024-05-14T20:31:08.199957Z","shell.execute_reply.started":"2024-05-14T20:31:08.092417Z","shell.execute_reply":"2024-05-14T20:31:08.198896Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"     image_name                                               text  \\\n0   image_1.jpg  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n1  image_2.jpeg  The best of #10 YearChallenge! Completed in le...   \n2   image_3.JPG  Sam Thorne @Strippin ( Follow Follow Saw every...   \n3   image_4.png              10 Year Challenge - Sweet Dee Edition   \n4   image_5.png  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   \n\n            label  \n0   not_offensive  \n1   not_offensive  \n2   not_offensive  \n3  very_offensive  \n4  very_offensive  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>image_1.jpg</td>\n      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>image_2.jpeg</td>\n      <td>The best of #10 YearChallenge! Completed in le...</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>image_3.JPG</td>\n      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>image_4.png</td>\n      <td>10 Year Challenge - Sweet Dee Edition</td>\n      <td>very_offensive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>image_5.png</td>\n      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n      <td>very_offensive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"labelDict = {'1(slightly)': 'slight', '0(non-offensive)':'not_offensive', '3(very)':'hateful_offensive', '2(moderately)':'very_offensive'}","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:31:08.201375Z","iopub.execute_input":"2024-05-14T20:31:08.201716Z","iopub.status.idle":"2024-05-14T20:31:08.207057Z","shell.execute_reply.started":"2024-05-14T20:31:08.201688Z","shell.execute_reply":"2024-05-14T20:31:08.205642Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"data2 = pd.read_csv(f\"{ROOT_PATH2}/E_text.csv\", encoding = 'latin-1')\nlabelsDF = pd.read_csv(f\"{ROOT_PATH2}/label_E.csv\", encoding = 'latin-1')\nlabelsDF['offensiveness detection'] = labelsDF['offensiveness detection'].map(labelDict)\nlabelsDF.drop(columns=[\"sentiment category\",\"sentiment degree\", \"intention detection\", \"metaphor occurrence\", \n                       \"metaphor category\", \"target domain\", \"source domain\", \"target modality\", \"source modality\"], inplace=True)\nlabelsDF.columns = ['file_name', 'label']\n\ndata2.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:31:08.208222Z","iopub.execute_input":"2024-05-14T20:31:08.208506Z","iopub.status.idle":"2024-05-14T20:31:08.261052Z","shell.execute_reply.started":"2024-05-14T20:31:08.208480Z","shell.execute_reply":"2024-05-14T20:31:08.260177Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"          file_name                                               text\n0    image_ (0).jpg  That moment after you throw up and your friend...\n1    image_ (1).jpg       EVERYDAY IS CATURDAY IF you're a cat! I CATS\n2   image_ (10).jpg                              me\\nfood at a potluck\n3  image_ (100).jpg  My knowledge in love vs. My knowledge in theor...\n4  image_ (101).jpg  My knowledge of Star   \\n Wars My knowledge of...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>image_ (0).jpg</td>\n      <td>That moment after you throw up and your friend...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>image_ (1).jpg</td>\n      <td>EVERYDAY IS CATURDAY IF you're a cat! I CATS</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>image_ (10).jpg</td>\n      <td>me\\nfood at a potluck</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>image_ (100).jpg</td>\n      <td>My knowledge in love vs. My knowledge in theor...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>image_ (101).jpg</td>\n      <td>My knowledge of Star   \\n Wars My knowledge of...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"labelsDF.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:31:08.262190Z","iopub.execute_input":"2024-05-14T20:31:08.262791Z","iopub.status.idle":"2024-05-14T20:31:08.271570Z","shell.execute_reply.started":"2024-05-14T20:31:08.262761Z","shell.execute_reply":"2024-05-14T20:31:08.270687Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"          file_name          label\n0    image_ (0).jpg         slight\n1    image_ (1).jpg  not_offensive\n2   image_ (10).jpg  not_offensive\n3  image_ (100).jpg         slight\n4  image_ (101).jpg  not_offensive","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>file_name</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>image_ (0).jpg</td>\n      <td>slight</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>image_ (1).jpg</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>image_ (10).jpg</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>image_ (100).jpg</td>\n      <td>slight</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>image_ (101).jpg</td>\n      <td>not_offensive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"dataset2 = pd.merge(data2, labelsDF, on='file_name')\ndataset2.columns = ['image_name', 'text', 'label']\ndataset2","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:31:08.272808Z","iopub.execute_input":"2024-05-14T20:31:08.273185Z","iopub.status.idle":"2024-05-14T20:31:08.309714Z","shell.execute_reply.started":"2024-05-14T20:31:08.273158Z","shell.execute_reply":"2024-05-14T20:31:08.308870Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"            image_name                                               text  \\\n0       image_ (0).jpg  That moment after you throw up and your friend...   \n1       image_ (1).jpg       EVERYDAY IS CATURDAY IF you're a cat! I CATS   \n2      image_ (10).jpg                              me\\nfood at a potluck   \n3     image_ (100).jpg  My knowledge in love vs. My knowledge in theor...   \n4     image_ (101).jpg  My knowledge of Star   \\n Wars My knowledge of...   \n...                ...                                                ...   \n3989  image_ (995).jpg     WHERE TO?\\nDON'T YOU EVER DISINFECT THIS TAXI?   \n3990  image_ (996).jpg  you remember the first few weeks we were looki...   \n3991  image_ (997).jpg  A neutron walks into a bar and asks how much f...   \n3992  image_ (998).jpg  WHAT IF THE BIG BANG WAS A RESET BUTTON FOR A ...   \n3993  image_ (999).jpg  DISMISSES THE BIG BANG THEORY AS PARTICLES CAN...   \n\n              label  \n0            slight  \n1     not_offensive  \n2     not_offensive  \n3            slight  \n4     not_offensive  \n...             ...  \n3989  not_offensive  \n3990         slight  \n3991  not_offensive  \n3992  not_offensive  \n3993         slight  \n\n[3994 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>image_ (0).jpg</td>\n      <td>That moment after you throw up and your friend...</td>\n      <td>slight</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>image_ (1).jpg</td>\n      <td>EVERYDAY IS CATURDAY IF you're a cat! I CATS</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>image_ (10).jpg</td>\n      <td>me\\nfood at a potluck</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>image_ (100).jpg</td>\n      <td>My knowledge in love vs. My knowledge in theor...</td>\n      <td>slight</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>image_ (101).jpg</td>\n      <td>My knowledge of Star   \\n Wars My knowledge of...</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>3989</th>\n      <td>image_ (995).jpg</td>\n      <td>WHERE TO?\\nDON'T YOU EVER DISINFECT THIS TAXI?</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>3990</th>\n      <td>image_ (996).jpg</td>\n      <td>you remember the first few weeks we were looki...</td>\n      <td>slight</td>\n    </tr>\n    <tr>\n      <th>3991</th>\n      <td>image_ (997).jpg</td>\n      <td>A neutron walks into a bar and asks how much f...</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>3992</th>\n      <td>image_ (998).jpg</td>\n      <td>WHAT IF THE BIG BANG WAS A RESET BUTTON FOR A ...</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>3993</th>\n      <td>image_ (999).jpg</td>\n      <td>DISMISSES THE BIG BANG THEORY AS PARTICLES CAN...</td>\n      <td>slight</td>\n    </tr>\n  </tbody>\n</table>\n<p>3994 rows × 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"data = pd.concat([data1, dataset2])\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:31:08.310813Z","iopub.execute_input":"2024-05-14T20:31:08.311156Z","iopub.status.idle":"2024-05-14T20:31:08.322544Z","shell.execute_reply.started":"2024-05-14T20:31:08.311126Z","shell.execute_reply":"2024-05-14T20:31:08.321341Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"     image_name                                               text  \\\n0   image_1.jpg  LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...   \n1  image_2.jpeg  The best of #10 YearChallenge! Completed in le...   \n2   image_3.JPG  Sam Thorne @Strippin ( Follow Follow Saw every...   \n3   image_4.png              10 Year Challenge - Sweet Dee Edition   \n4   image_5.png  10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...   \n\n            label  \n0   not_offensive  \n1   not_offensive  \n2   not_offensive  \n3  very_offensive  \n4  very_offensive  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>text</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>image_1.jpg</td>\n      <td>LOOK THERE MY FRIEND LIGHTYEAR NOW ALL SOHALIK...</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>image_2.jpeg</td>\n      <td>The best of #10 YearChallenge! Completed in le...</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>image_3.JPG</td>\n      <td>Sam Thorne @Strippin ( Follow Follow Saw every...</td>\n      <td>not_offensive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>image_4.png</td>\n      <td>10 Year Challenge - Sweet Dee Edition</td>\n      <td>very_offensive</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>image_5.png</td>\n      <td>10 YEAR CHALLENGE WITH NO FILTER 47 Hilarious ...</td>\n      <td>very_offensive</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import os\nimport shutil\n\nNEW_ROOT_PATH = \"/kaggle/working/data_images\"\n\nif os.path.exists(NEW_ROOT_PATH):\n    # Use os.rmdir() to remove an empty directory\n    shutil.rmtree(NEW_ROOT_PATH)\n    print(\"Directory removed successfully.\")\n\nos.mkdir(NEW_ROOT_PATH)\n\nsource_folder1 = \"/kaggle/input/memotion-dataset-7k/memotion_dataset_7k/images\"\nsource_folder2 = \"/kaggle/input/met-meme/Eimages/Eimages/Eimages\"\n\ndef copy_images(source_folder, destination_folder):\n    for filename in os.listdir(source_folder):\n        shutil.copy(os.path.join(source_folder, filename), destination_folder)\n\n# Copy images from source_folder1\ncopy_images(source_folder1, NEW_ROOT_PATH)\n\n# Copy images from source_f`older2\ncopy_images(source_folder2, NEW_ROOT_PATH)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:31:08.324474Z","iopub.execute_input":"2024-05-14T20:31:08.324835Z","iopub.status.idle":"2024-05-14T20:32:22.573251Z","shell.execute_reply.started":"2024-05-14T20:31:08.324803Z","shell.execute_reply":"2024-05-14T20:32:22.572351Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# os.listdir('/kaggle/working/data_images')","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:32:22.575476Z","iopub.execute_input":"2024-05-14T20:32:22.575763Z","iopub.status.idle":"2024-05-14T20:32:22.580074Z","shell.execute_reply.started":"2024-05-14T20:32:22.575738Z","shell.execute_reply":"2024-05-14T20:32:22.578978Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Plot Distribution\n> Plotting the distribution of Labels","metadata":{}},{"cell_type":"code","source":"def plot_distribution(df, column_name):\n    \n    Distribution = df[column_name].value_counts().to_dict()\n    categories = list(Distribution.keys())\n    values = list(Distribution.values())\n\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=categories, y=values, palette='Set2')\n    for i, v in enumerate(values):\n        plt.text(i, v + 50, str(v), ha='center', va='bottom')\n    plt.xlabel('Offensiveness Categories')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Offensiveness Categories')\n    plt.xticks(rotation=45)  # Rotating x-axis labels for better readability\n    plt.tight_layout()  # Adjust layout to prevent clipping of labels\n    plt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distribution(data, \"label\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Balancing the Dataset\n\n> merged \"very_offensive\" & \"hateful_offensive\" as 'very_offensive'","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Merge 'very_offensive' and 'hateful_offensive' into one category\ndata['label'] = data['label'].replace({ 'hateful_offensive': 'very_offensive'})\nplot_distribution(data, \"label\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### One Hot Encoding the lables","metadata":{}},{"cell_type":"code","source":"# # Perform one-hot encoding\n# train_df = pd.get_dummies(train_df, columns=['label'], prefix = \"\", \n#                             prefix_sep = \"\",dtype = \"float32\")\n\n# val_df = pd.get_dummies(val_df, columns=['label'], prefix = \"\", \n#                             prefix_sep = \"\",dtype = \"float32\")\n\n# test_df = pd.get_dummies(test_df, columns=['label'], prefix = \"\", \n#                             prefix_sep = \"\",dtype = \"float32\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import AutoModel, AutoTokenizer, get_scheduler\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import AdamW, SGD\nfrom tqdm.notebook import tqdm, trange\nfrom time import perf_counter\nfrom PIL import Image\nimport pandas as pd","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:32:22.581291Z","iopub.execute_input":"2024-05-14T20:32:22.581568Z","iopub.status.idle":"2024-05-14T20:32:22.596695Z","shell.execute_reply.started":"2024-05-14T20:32:22.581544Z","shell.execute_reply":"2024-05-14T20:32:22.595885Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:32:41.924569Z","iopub.execute_input":"2024-05-14T20:32:41.924975Z","iopub.status.idle":"2024-05-14T20:32:41.977923Z","shell.execute_reply.started":"2024-05-14T20:32:41.924945Z","shell.execute_reply":"2024-05-14T20:32:41.976847Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"device(type='cuda')"},"metadata":{}}]},{"cell_type":"code","source":"!pip install wget","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:32:43.061244Z","iopub.execute_input":"2024-05-14T20:32:43.061616Z","iopub.status.idle":"2024-05-14T20:32:58.593607Z","shell.execute_reply.started":"2024-05-14T20:32:43.061587Z","shell.execute_reply":"2024-05-14T20:32:58.592319Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"Collecting wget\n  Downloading wget-3.2.zip (10 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hBuilding wheels for collected packages: wget\n  Building wheel for wget (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9655 sha256=3b1d1decd006314f180eef06912bd5334dbdb588907a40b87b561770d9d2bfdb\n  Stored in directory: /root/.cache/pip/wheels/8b/f1/7f/5c94f0a7a505ca1c81cd1d9208ae2064675d97582078e6c769\nSuccessfully built wget\nInstalling collected packages: wget\nSuccessfully installed wget-3.2\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nALBEF_FOLDER = 'ALBEF/'\nos.makedirs(ALBEF_FOLDER, exist_ok=True)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:32:58.596108Z","iopub.execute_input":"2024-05-14T20:32:58.596531Z","iopub.status.idle":"2024-05-14T20:32:58.603057Z","shell.execute_reply.started":"2024-05-14T20:32:58.596495Z","shell.execute_reply":"2024-05-14T20:32:58.601599Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# download pre-trained ALBEF model and required ALBEF files from ALBEF's official repo (only need to do this once to save it in your gdrive)\n!wget https://raw.githubusercontent.com/salesforce/ALBEF/main/models/vit.py -O $ALBEF_FOLDER/vit.py\n!wget https://raw.githubusercontent.com/salesforce/ALBEF/main/models/tokenization_bert.py -O $ALBEF_FOLDER/tokenization_bert.py\n!wget https://raw.githubusercontent.com/salesforce/ALBEF/main/models/xbert.py -O $ALBEF_FOLDER/xbert.py\n","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:32:58.604687Z","iopub.execute_input":"2024-05-14T20:32:58.605079Z","iopub.status.idle":"2024-05-14T20:33:02.036854Z","shell.execute_reply.started":"2024-05-14T20:32:58.605044Z","shell.execute_reply":"2024-05-14T20:33:02.035444Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"--2024-05-14 20:32:59--  https://raw.githubusercontent.com/salesforce/ALBEF/main/models/vit.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 8558 (8.4K) [text/plain]\nSaving to: ‘ALBEF//vit.py’\n\nALBEF//vit.py       100%[===================>]   8.36K  --.-KB/s    in 0s      \n\n2024-05-14 20:32:59 (52.1 MB/s) - ‘ALBEF//vit.py’ saved [8558/8558]\n\n--2024-05-14 20:33:00--  https://raw.githubusercontent.com/salesforce/ALBEF/main/models/tokenization_bert.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 24750 (24K) [text/plain]\nSaving to: ‘ALBEF//tokenization_bert.py’\n\nALBEF//tokenization 100%[===================>]  24.17K  --.-KB/s    in 0.002s  \n\n2024-05-14 20:33:00 (15.2 MB/s) - ‘ALBEF//tokenization_bert.py’ saved [24750/24750]\n\n--2024-05-14 20:33:01--  https://raw.githubusercontent.com/salesforce/ALBEF/main/models/xbert.py\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 82210 (80K) [text/plain]\nSaving to: ‘ALBEF//xbert.py’\n\nALBEF//xbert.py     100%[===================>]  80.28K  --.-KB/s    in 0.02s   \n\n2024-05-14 20:33:01 (3.58 MB/s) - ‘ALBEF//xbert.py’ saved [82210/82210]\n\n","output_type":"stream"}]},{"cell_type":"code","source":"# replace all occurrences of tokenizer_class with processor_class in xbert.py to make it compatible with newer transformers version\n# if you don't do this step, you will need to install transformers==4.8.1 as specified by the requirements in the ALBEF repo\n\n!sed -i 's/tokenizer_class/processor_class/g' $ALBEF_FOLDER/xbert.py","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:33:02.039289Z","iopub.execute_input":"2024-05-14T20:33:02.039614Z","iopub.status.idle":"2024-05-14T20:33:03.000705Z","shell.execute_reply.started":"2024-05-14T20:33:02.039586Z","shell.execute_reply":"2024-05-14T20:33:02.999387Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# add path to downloaded ALBEF files\nimport sys\nsys.path.append(ALBEF_FOLDER)\n\n#import libraries required for ALBEF\nfrom vit import VisionTransformer\nfrom xbert import BertConfig as AlbefBertConfig, BertModel as AlbefBertModel\nfrom functools import partial\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:33:03.002218Z","iopub.execute_input":"2024-05-14T20:33:03.002524Z","iopub.status.idle":"2024-05-14T20:33:04.378182Z","shell.execute_reply.started":"2024-05-14T20:33:03.002497Z","shell.execute_reply":"2024-05-14T20:33:04.377201Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"data = data.dropna(axis=0)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:33:04.379357Z","iopub.execute_input":"2024-05-14T20:33:04.379647Z","iopub.status.idle":"2024-05-14T20:33:04.390193Z","shell.execute_reply.started":"2024-05-14T20:33:04.379621Z","shell.execute_reply":"2024-05-14T20:33:04.389306Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"import pickle\ndef load_all_images_and_text(df, path):\n    images_paths = df['image_name'].to_list()\n    text = df['text'].to_list()\n    texts = []\n    label = df['label'].to_list()\n    labels = []\n    images = []\n    for ind, filename in enumerate(images_paths):\n        try:\n            img = Image.open(os.path.join(path,filename)).convert('RGB')\n        except:\n            continue\n        images.append(img)\n        texts.append(text[ind])\n        labels.append(label[ind])\n\n    return texts, images, labels\n\ntext, images, labels = load_all_images_and_text(data, path = NEW_ROOT_PATH)\n\nwith open(\"text\", \"wb\") as fp:\n    pickle.dump(text, fp)\n\nwith open(\"images\", \"wb\") as fp:\n    pickle.dump(images, fp)\n\nwith open(\"labels\", \"wb\") as fp:\n    pickle.dump(labels, fp)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:33:08.935217Z","iopub.execute_input":"2024-05-14T20:33:08.935999Z","iopub.status.idle":"2024-05-14T20:34:47.698139Z","shell.execute_reply.started":"2024-05-14T20:33:08.935960Z","shell.execute_reply":"2024-05-14T20:34:47.697032Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"len(images)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:34:52.259422Z","iopub.execute_input":"2024-05-14T20:34:52.260472Z","iopub.status.idle":"2024-05-14T20:34:52.266457Z","shell.execute_reply.started":"2024-05-14T20:34:52.260433Z","shell.execute_reply":"2024-05-14T20:34:52.265557Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"10922"},"metadata":{}}]},{"cell_type":"code","source":"len(text)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:34:53.975818Z","iopub.execute_input":"2024-05-14T20:34:53.976770Z","iopub.status.idle":"2024-05-14T20:34:53.983299Z","shell.execute_reply.started":"2024-05-14T20:34:53.976733Z","shell.execute_reply":"2024-05-14T20:34:53.982185Z"},"trusted":true},"execution_count":22,"outputs":[{"execution_count":22,"output_type":"execute_result","data":{"text/plain":"10922"},"metadata":{}}]},{"cell_type":"code","source":"len(labels)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:34:55.014457Z","iopub.execute_input":"2024-05-14T20:34:55.014810Z","iopub.status.idle":"2024-05-14T20:34:55.020770Z","shell.execute_reply.started":"2024-05-14T20:34:55.014782Z","shell.execute_reply":"2024-05-14T20:34:55.019845Z"},"trusted":true},"execution_count":23,"outputs":[{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"10922"},"metadata":{}}]},{"cell_type":"code","source":"class AlbefDataset(Dataset):\n    def __init__(self, text, images, label):\n        self.text = text\n        self.images = images\n        self.label = label\n\n        # ALBEF settings\n        self.img_size = 256\n        self.mean, self.std = (\n            0.40821073), (0.26862954)\n\n\n        self.train_transform_func = transforms.Compose(\n                [transforms.RandomResizedCrop(self.img_size, scale=(0.5, 1.0)),\n                    transforms.RandomHorizontalFlip(),\n                    transforms.ToTensor(),\n                    transforms.Normalize(self.mean, self.std)\n                    ])\n\n                    \n    def __getitem__(self, idx):\n        text = self.text[idx]\n        label = self.label[idx]\n        if label == 'not_offensive':\n            label = 0\n        elif label == 'very_offensive':\n            label = 1\n        else:\n            label = 2\n        image = self.images[idx]\n        \n        \n        img = self.train_transform_func(image)\n\n        return text, img, label\n\n    def __len__(self):\n        return len(self.text)\n\ndataset = AlbefDataset(text, images, labels)\ndataset[2]","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:34:56.937193Z","iopub.execute_input":"2024-05-14T20:34:56.938155Z","iopub.status.idle":"2024-05-14T20:34:57.255254Z","shell.execute_reply.started":"2024-05-14T20:34:56.938119Z","shell.execute_reply":"2024-05-14T20:34:57.254262Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"(\"Sam Thorne @Strippin ( Follow Follow Saw everyone posting these 2009 vs 2019 pics so here's mine 6:23 PM - 12 Jan 2019 O 636 Retweets 3 224 LIKES 65 636 3.2K\",\n tensor([[[-1.1108, -1.1254, -1.1400,  ...,  2.2030,  2.2030,  2.2030],\n          [-1.1108, -1.1254, -1.1400,  ...,  2.2030,  2.2030,  2.2030],\n          [-1.1108, -1.1108, -1.1254,  ...,  2.2030,  2.2030,  2.2030],\n          ...,\n          [ 0.1446,  0.1446,  0.1446,  ...,  2.2030,  2.2030,  2.2030],\n          [ 0.1446,  0.1446,  0.1446,  ...,  2.2030,  2.2030,  2.2030],\n          [ 0.1446,  0.1446,  0.1446,  ...,  2.2030,  2.2030,  2.2030]],\n \n         [[-0.9357, -0.9503, -0.9503,  ...,  2.2030,  2.2030,  2.2030],\n          [-0.9357, -0.9503, -0.9503,  ...,  2.2030,  2.2030,  2.2030],\n          [-0.9357, -0.9503, -0.9357,  ...,  2.2030,  2.2030,  2.2030],\n          ...,\n          [ 0.2030,  0.2030,  0.2030,  ...,  2.2030,  2.2030,  2.2030],\n          [ 0.2030,  0.2030,  0.2030,  ...,  2.2030,  2.2030,  2.2030],\n          [ 0.2030,  0.2030,  0.2030,  ...,  2.2030,  2.2030,  2.2030]],\n \n         [[-0.4393, -0.4539, -0.4539,  ...,  2.2030,  2.2030,  2.2030],\n          [-0.4393, -0.4539, -0.4539,  ...,  2.2030,  2.2030,  2.2030],\n          [-0.4393, -0.4393, -0.4393,  ...,  2.2030,  2.2030,  2.2030],\n          ...,\n          [ 0.2468,  0.2468,  0.2468,  ...,  2.2030,  2.2030,  2.2030],\n          [ 0.2468,  0.2468,  0.2468,  ...,  2.2030,  2.2030,  2.2030],\n          [ 0.2468,  0.2468,  0.2468,  ...,  2.2030,  2.2030,  2.2030]]]),\n 0)"},"metadata":{}}]},{"cell_type":"code","source":"bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:34:58.965282Z","iopub.execute_input":"2024-05-14T20:34:58.965651Z","iopub.status.idle":"2024-05-14T20:35:00.095596Z","shell.execute_reply.started":"2024-05-14T20:34:58.965621Z","shell.execute_reply":"2024-05-14T20:35:00.094848Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"25ab6b0d06894612b76ab9ed26b35644"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b5a782d868749c6995a18d777e68bd6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a26c21783e4f4c3ea7c61dba1ab6be04"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12d50f77c0204016b98bcd00922da2a9"}},"metadata":{}}]},{"cell_type":"code","source":"class AlbefModel(nn.Module):\n\n    def __init__(self, bert_config, num_labels, text_pretrained='bert-base-uncased'):\n        super().__init__()\n\n        self.num_labels = num_labels\n        self.text_encoder = AlbefBertModel.from_pretrained(\n            text_pretrained, config=bert_config, add_pooling_layer=False)\n\n        self.visual_encoder = VisionTransformer(\n            img_size=256, patch_size=16, embed_dim=768, depth=12, num_heads=12,\n            mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n\n        self.classifier = nn.Linear(\n            self.text_encoder.config.hidden_size, num_labels)\n        \n    \n    def forward(self, text, image):\n        image_embeds = self.visual_encoder(image)\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image_embeds.device)\n        output = self.text_encoder(text.input_ids, attention_mask=text.attention_mask,\n                                   encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, return_dict=True\n                                   )\n        logits = self.classifier(output.last_hidden_state[:, 0, :])\n        return logits","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:35:00.392494Z","iopub.execute_input":"2024-05-14T20:35:00.392821Z","iopub.status.idle":"2024-05-14T20:35:00.401207Z","shell.execute_reply.started":"2024-05-14T20:35:00.392793Z","shell.execute_reply":"2024-05-14T20:35:00.400286Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"from urllib.request import urlretrieve\n\ndef load_albef_pretrained(num_out_labels):\n    tmp_directory = './tmp/albef'\n    os.makedirs(tmp_directory, exist_ok=True)\n\n    albef_bert_config_fp = os.path.join(tmp_directory, 'config_bert.json')\n    albef_model_fp = os.path.join(tmp_directory, 'ALBEF.pth')\n\n    if not os.path.exists(albef_bert_config_fp):\n        urlretrieve(\"https://raw.githubusercontent.com/salesforce/ALBEF/main/configs/config_bert.json\", albef_bert_config_fp)\n\n    if not os.path.exists(albef_model_fp):\n        urlretrieve(\"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF_4M.pth\", albef_model_fp)\n\n    albef_bert_config = AlbefBertConfig.from_json_file(albef_bert_config_fp)\n    albef_model = AlbefModel(bert_config=albef_bert_config, num_labels=num_out_labels)\n\n    print(albef_model_fp)\n    albef_checkpoint = torch.load(albef_model_fp, map_location='cpu')\n    albef_state_dict = albef_checkpoint['model']\n\n    for key in list(albef_state_dict.keys()):\n        if 'bert' in key:\n            encoder_key = key.replace('bert.', '')\n            albef_state_dict[encoder_key] = albef_state_dict[key]\n            del albef_state_dict[key]\n\n    msg = albef_model.load_state_dict(albef_state_dict, strict=False)\n    print(\"ALBEF checkpoint loaded from \", albef_model_fp)\n    # print(msg)\n    return albef_model","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:35:01.967024Z","iopub.execute_input":"2024-05-14T20:35:01.967829Z","iopub.status.idle":"2024-05-14T20:35:01.976608Z","shell.execute_reply.started":"2024-05-14T20:35:01.967790Z","shell.execute_reply":"2024-05-14T20:35:01.975585Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"albef_model = load_albef_pretrained(num_out_labels=1)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:35:08.933804Z","iopub.execute_input":"2024-05-14T20:35:08.934586Z","iopub.status.idle":"2024-05-14T20:35:34.222350Z","shell.execute_reply.started":"2024-05-14T20:35:08.934552Z","shell.execute_reply":"2024-05-14T20:35:34.221341Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27a8682aafc84d49884f63267a46cb5d"}},"metadata":{}},{"name":"stdout","text":"./tmp/albef/ALBEF.pth\nALBEF checkpoint loaded from  ./tmp/albef/ALBEF.pth\n","output_type":"stream"}]},{"cell_type":"code","source":"albef_model.parameters()","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:35:34.224501Z","iopub.execute_input":"2024-05-14T20:35:34.225324Z","iopub.status.idle":"2024-05-14T20:35:34.231237Z","shell.execute_reply.started":"2024-05-14T20:35:34.225284Z","shell.execute_reply":"2024-05-14T20:35:34.230189Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"<generator object Module.parameters at 0x7f51687da260>"},"metadata":{}}]},{"cell_type":"code","source":"from transformers import AutoImageProcessor\nimport numpy as np\n\nb_inputs = bert_tokenizer(\n            \"not hello\", truncation=True, max_length=512,\n            return_tensors=\"pt\", padding='max_length'\n        ).to(device)\n\n# b_inputs = b_inputs.to(device)\nb_imgs = Image.open(NEW_ROOT_PATH + \"/image_445.jpg\")\nb_imgs = b_imgs.resize((256,256))\nb_imgs = torch.Tensor(np.array(b_imgs)).unsqueeze(0).permute(0, 3, 1, 2).to(device)\nb_inputs, b_imgs.shape\nb_imgs = b_imgs.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:35:37.009337Z","iopub.execute_input":"2024-05-14T20:35:37.009996Z","iopub.status.idle":"2024-05-14T20:35:48.472087Z","shell.execute_reply.started":"2024-05-14T20:35:37.009963Z","shell.execute_reply":"2024-05-14T20:35:48.470840Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# x = albef_model(b_inputs, b_imgs)\n# x","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:35:48.473899Z","iopub.execute_input":"2024-05-14T20:35:48.474547Z","iopub.status.idle":"2024-05-14T20:35:48.481094Z","shell.execute_reply.started":"2024-05-14T20:35:48.474517Z","shell.execute_reply":"2024-05-14T20:35:48.479937Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# import time\n\n# # Start timer\n# start_time = time.perf_counter()\n# x = albef_model(b_inputs, b_imgs)\n# x.mean().backward()\n\n# end_time = time.perf_counter()\n\n# # Calculate elapsed time\n# elapsed_time = end_time - start_time\n# print(\"Elapsed time: \", elapsed_time)","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:35:48.482982Z","iopub.execute_input":"2024-05-14T20:35:48.483306Z","iopub.status.idle":"2024-05-14T20:35:48.500315Z","shell.execute_reply.started":"2024-05-14T20:35:48.483274Z","shell.execute_reply":"2024-05-14T20:35:48.499484Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"train_data_loader = torch.utils.data.DataLoader(dataset, batch_size = 16, shuffle = True)\ntrain_data_loader","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:35:48.502414Z","iopub.execute_input":"2024-05-14T20:35:48.502849Z","iopub.status.idle":"2024-05-14T20:35:48.517952Z","shell.execute_reply.started":"2024-05-14T20:35:48.502808Z","shell.execute_reply":"2024-05-14T20:35:48.516996Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"<torch.utils.data.dataloader.DataLoader at 0x7f51682164a0>"},"metadata":{}}]},{"cell_type":"code","source":"from torchvision.ops import sigmoid_focal_loss\nnum_train_epochs = 5\nlearning_rate = 0.0005\nt_total = len(train_data_loader) * num_train_epochs\n\n\noptimizer = SGD(albef_model.parameters(), lr=learning_rate)\n# scheduler = get_scheduler(name=\"cosine\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=t_total)\n\ncriterion = sigmoid_focal_loss","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:35:48.519090Z","iopub.execute_input":"2024-05-14T20:35:48.519427Z","iopub.status.idle":"2024-05-14T20:35:48.529866Z","shell.execute_reply.started":"2024-05-14T20:35:48.519394Z","shell.execute_reply":"2024-05-14T20:35:48.529159Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nalbef_model = albef_model.to(device)\nalbef_model.train()\n\nfor epoch_num in trange(num_train_epochs, desc='Epochs'):\n    epoch_total_loss = 0\n\n    for step, batch in tqdm(enumerate(train_data_loader), total=len(train_data_loader), desc='Batch'):        \n        b_text, b_imgs, b_labels, = batch  \n        # print(b_text, b_imgs, b_labels)\n        # break\n        b_inputs = bert_tokenizer(\n            list(b_text), truncation=True, max_length=128,\n            return_tensors=\"pt\", padding='max_length'\n        )\n        \n        b_labels = b_labels.to(device)\n        b_imgs = b_imgs.to(device)\n        b_inputs = b_inputs.to(device)\n\n        albef_model.zero_grad()        \n        b_logits = albef_model(text=b_inputs, image=b_imgs).squeeze(1)\n        \n        loss = criterion(b_logits.to(torch.float32), b_labels.to(torch.float32), reduction = 'mean')\n\n        epoch_total_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients\n        loss.backward()\n\n\n        optimizer.step()\n        # scheduler.step()\n        \n    avg_loss = epoch_total_loss/len(train_data_loader)\n\n\n    print('epoch =', epoch_num)\n    print('    epoch_loss =', epoch_total_loss)\n    print('    avg_epoch_loss =', avg_loss)\n    print('    learning rate =', optimizer.param_groups[0][\"lr\"])","metadata":{"execution":{"iopub.status.busy":"2024-05-14T20:35:52.895555Z","iopub.execute_input":"2024-05-14T20:35:52.896044Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epochs:   0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c4a8c97e0e7e4e6797b8ba55cde3f22e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batch:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3787a2f29411431c9fb0030b90e915f3"}},"metadata":{}},{"name":"stdout","text":"epoch = 0\n    epoch_loss = -14656.630869079381\n    avg_epoch_loss = -21.459196001580352\n    learning rate = 0.0005\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batch:   0%|          | 0/683 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2a06af167f648589471f2349fc78e20"}},"metadata":{}}]},{"cell_type":"code","source":"b_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}