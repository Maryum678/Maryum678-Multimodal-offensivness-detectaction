{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":973292,"sourceType":"datasetVersion","datasetId":531544},{"sourceId":6011029,"sourceType":"datasetVersion","datasetId":2638370}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### To do\n","metadata":{}},{"cell_type":"markdown","source":"> Two datasets \"MET-Meme & Memotion\" (Multimodal image and text) offensive Meme detection \n\n> \"Multiclass: Not offensive, slight offensive and very offensive\" \n\n> Two Models Models are parallelly work: \n    on text (BERT and CNN) on images (RESNet-152, also comment ResNet-50) \n    Then Dense layer, fully concatenation layer and final multi classification (softmax). \n    Requirement is Dataset should be 7k or more also balance dataset. \n    Accuracy, recall, f1score, precision (validation and training loss graph), \n    (validation and training accuracy graph), (Confusion Matrix)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"**Libraries**","metadata":{}},{"cell_type":"code","source":"rm -rf *pkl","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport numpy as np\nimport pandas as pd\n\nfrom PIL import Image\nimport cv2 \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport pickle\nimport os\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom tqdm import tqdm\n\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, random_split\nfrom torchvision import transforms\nfrom transformers import BertTokenizer, BertModel\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport torchvision.models as models\nimport torch.optim as optim\nfrom torchvision.models import resnet152\nfrom transformers import BertModel, BertTokenizer","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA and Preprocessing","metadata":{}},{"cell_type":"markdown","source":"### *Dataset Path*","metadata":{}},{"cell_type":"code","source":"ROOT_PATH = \"/kaggle/input/memotion-dataset-7k/memotion_dataset_7k\"\nROOT_PATH2 = \"/kaggle/input/met-meme\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data1 = pd.read_csv(f\"{ROOT_PATH}/labels.csv\")\n# data.drop(columns= [\"Unnamed: 0\", \"text_ocr\", \"humour\", \"sarcasm\", \"motivational\", \"overall_sentiment\"],\n#             inplace = True)\n# data.columns = [\"image_name\", \"text\", \"label\"]\n\n\ndata1.drop(columns= [\"Unnamed: 0\",\"text_ocr\", 'humour', 'sarcasm', 'motivational' , 'overall_sentiment'], inplace = True)\ndata1.columns = [\"image_name\", \"text\", \"label\"]\n\ndata1.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelDict = {'1(slightly)': 'slight', '0(non-offensive)':'not_offensive', '3(very)':'hateful_offensive', '2(moderately)':'very_offensive'}","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data2 = pd.read_csv(f\"{ROOT_PATH2}/E_text.csv\", encoding = 'latin-1')\nlabelsDF = pd.read_csv(f\"{ROOT_PATH2}/label_E.csv\", encoding = 'latin-1')\nlabelsDF['offensiveness detection'] = labelsDF['offensiveness detection'].map(labelDict)\nlabelsDF.drop(columns=[\"sentiment category\",\"sentiment degree\", \"intention detection\", \"metaphor occurrence\", \n                       \"metaphor category\", \"target domain\", \"source domain\", \"target modality\", \"source modality\"], inplace=True)\nlabelsDF.columns = ['file_name', 'label']\n\ndata2.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labelsDF.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset2 = pd.merge(data2, labelsDF, on='file_name')\ndataset2.columns = ['image_name', 'text', 'label']\ndataset2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.concat([data1, dataset2])\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport shutil\n\nNEW_ROOT_PATH = \"/kaggle/working/data_images\"\n\nif os.path.exists(NEW_ROOT_PATH):\n    # Use os.rmdir() to remove an empty directory\n    shutil.rmtree(NEW_ROOT_PATH)\n    print(\"Directory removed successfully.\")\n\nos.mkdir(NEW_ROOT_PATH)\n\nsource_folder1 = \"/kaggle/input/memotion-dataset-7k/memotion_dataset_7k/images\"\nsource_folder2 = \"/kaggle/input/met-meme/Eimages/Eimages/Eimages\"\n\ndef copy_images(source_folder, destination_folder):\n    for filename in os.listdir(source_folder):\n        shutil.copy(os.path.join(source_folder, filename), destination_folder)\n\n# Copy images from source_folder1\ncopy_images(source_folder1, NEW_ROOT_PATH)\n\n# Copy images from source_f`older2\ncopy_images(source_folder2, NEW_ROOT_PATH)\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.listdir('/kaggle/working/data_images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plot Distribution\n> Plotting the distribution of Labels","metadata":{}},{"cell_type":"code","source":"def plot_distribution(df, column_name):\n    \n    Distribution = df[column_name].value_counts().to_dict()\n    categories = list(Distribution.keys())\n    values = list(Distribution.values())\n\n    plt.figure(figsize=(10, 6))\n    sns.barplot(x=categories, y=values, palette='Set2')\n    for i, v in enumerate(values):\n        plt.text(i, v + 50, str(v), ha='center', va='bottom')\n    plt.xlabel('Offensiveness Categories')\n    plt.ylabel('Frequency')\n    plt.title('Frequency of Offensiveness Categories')\n    plt.xticks(rotation=45)  # Rotating x-axis labels for better readability\n    plt.tight_layout()  # Adjust layout to prevent clipping of labels\n    plt.show()\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_distribution(data, \"label\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Balancing the Dataset\n\n> merged \"very_offensive\" & \"hateful_offensive\" as 'very_offensive'","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Merge 'very_offensive' and 'hateful_offensive' into one category\ndata['label'] = data['label'].replace({ 'hateful_offensive': 'very_offensive'})\nplot_distribution(data, \"label\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Splitting Data for Training, Validating and Testing\n\nEnsuring Equal Distribution of Labels in Each Subset","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport numpy as np\n\nX = data.drop(columns = [\"label\"])\ny = data[\"label\"]\n\n# Step 1: Shuffle the dataset\nindices = np.arange(len(X))\nnp.random.shuffle(indices)\nX_shuffled = X.iloc[indices]\ny_shuffled = y.iloc[indices]\n\n# Step 2: Calculate the percentage of each label\nunique_labels, label_counts = np.unique(y_shuffled, return_counts=True)\nlabel_percentages = label_counts / len(y_shuffled)\n\n# Step 3: Split the dataset while maintaining consistent label percentages\n# You can adjust the test_size and validation_size as needed\nX_train, X_temp, y_train, y_temp = train_test_split(X_shuffled, y_shuffled, test_size=0.2, stratify=y_shuffled)\nX_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp)\n\nX_train.reset_index(drop=True, inplace = True)\ny_train.reset_index(drop=True, inplace = True)\n\nX_validation.reset_index(drop=True, inplace = True)\ny_validation.reset_index(drop=True, inplace = True)\n\nX_test.reset_index(drop=True, inplace = True)\ny_test.reset_index(drop=True, inplace = True)\n\n# Check label percentages in each set\ntrain_label_percentage = np.unique(y_train, return_counts=True)[1] / len(y_train)\nvalidation_label_percentage = np.unique(y_validation, return_counts=True)[1] / len(y_validation)\ntest_label_percentage = np.unique(y_test, return_counts=True)[1] / len(y_test)\n\nprint(\"Label percentages in training set:\", train_label_percentage)\nprint(\"Label percentages in validation set:\", validation_label_percentage)\nprint(\"Label percentages in test set:\", test_label_percentage)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = pd.concat([X_train, y_train], axis=1)\nval_df = pd.concat([X_validation, y_validation], axis=1)\ntest_df = pd.concat([X_test, y_test], axis=1)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df.info()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### One Hot Encoding the lables","metadata":{}},{"cell_type":"code","source":"# Perform one-hot encoding\ntrain_df = pd.get_dummies(train_df, columns=['label'], prefix = \"\", \n                            prefix_sep = \"\",dtype = \"float32\")\n\nval_df = pd.get_dummies(val_df, columns=['label'], prefix = \"\", \n                            prefix_sep = \"\",dtype = \"float32\")\n\ntest_df = pd.get_dummies(test_df, columns=['label'], prefix = \"\", \n                            prefix_sep = \"\",dtype = \"float32\")\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Label Encoding","metadata":{}},{"cell_type":"code","source":"label_encoder = LabelEncoder()\n\n# # Apply LabelEncoder to each column\n# for column in [\"humour\", \"sarcasm\", \"motivational\", \"overall_sentiment\"]:\n#     train_df[column] = label_encoder.fit_transform(train_df[column])\n#     val_df[column] = label_encoder.fit_transform(val_df[column])\n#     test_df[column] = label_encoder.fit_transform(test_df[column])\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Making Dataset ","metadata":{}},{"cell_type":"code","source":"class MemotionDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, image_path: str, tokenizer, transforms, \n                 device :str = \"cpu\",mode :str = \"train\", pickle_name :str = \"data.pkl\"):\n        super().__init__()\n        self.df = df\n        self.imgPath = image_path\n        self.tokenizer = tokenizer\n        self.transforms = transforms\n        self.device = device\n        self.image_data = []\n        self.pickle_file = mode + \"_\" + pickle_name\n\n#         If pickle file doesn't exist, preprocess images and save to pickle file\n        if not os.path.exists(self.pickle_file):\n            self.preprocess_images()\n        else:\n            with open(self.pickle_file, 'rb') as f:\n                self.image_data = pickle.load(f)\n\n    def preprocess_images(self):\n        for ix, row in self.df.iterrows():\n            image_path = os.path.join(self.imgPath, row['image_name'])\n            try:\n                img = np.array(Image.open(image_path).convert('RGB'))\n                img = self.transforms(img)\n                text = str(row['text']).lower()\n                out = self.tokenizer(\n                    text=text,\n                    max_length=124,\n                    padding='max_length',\n                    truncation=True,\n                    return_tensors=\"pt\"\n                )\n#                 humour, sarcasm, motivational, overall_sentiment = row[[\"humour\", \"sarcasm\", \"motivational\", \"overall_sentiment\"]]\n                label = torch.tensor([row['not_offensive'], row['slight'], row['very_offensive']], dtype=torch.float32)\n                self.image_data.append({\n                    'image': img,\n                    'input_ids': torch.concat([out['input_ids'].squeeze()]), \n                    'attention_mask': torch.concat([out['attention_mask'].squeeze()]),\n                    'label': label,\n                })\n            except:\n                continue  # Skip\n\n        # Save image data to pickle file\n        with open(self.pickle_file, 'wb') as f:\n            pickle.dump(self.image_data, f)\n\n    def __len__(self) -> int:\n        return len(self.image_data)\n\n    def __getitem__(self, ix: int) -> dict[str, torch.Tensor]:\n        data = self.image_data[ix]\n        return {\n            'image': data['image'].to(self.device),\n            'input_ids': data['input_ids'].to(self.device),\n            'attention_mask': data['attention_mask'].to(self.device),\n            'label': data['label'].to(self.device),\n        }\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#         # Text model (BERT)\n#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n#         self.bert_out_features = bert_hidden_size\n\n#         # Image model (ResNet-152)\n#         self.resnet = resnet152(pretrained=True)\n#         self.resnet.fc = nn.Identity()  # We remove the final fully connected layer\n\n#         # Attention mechanism\n#         self.attention_fc = nn.Linear(bert_hidden_size + 2048, 1)\n\n#         # Final classification layer\n#         self.fc_layers = nn.Sequential(\n#             nn.Linear(bert_hidden_size + 2048, 128),\n#             nn.LayerNorm(128),\n#             nn.ReLU(),\n#             nn.Linear(128, num_classes),\n#         )\n\n#     def forward(self, input_ids, attention_mask, image):\n#         bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)[1]  # Taking pooled output\n#         resnet_output = self.resnet(image)\n#         print(\"bert:\", bert_output.size())\n#         print('resent:', resnet_output.size())\n\n#         # Concatenate BERT and ResNet outputs\n#         combined_features = torch.cat((bert_output, resnet_output), dim=1)\n#         print('comb:', combined_features.size())\n\n#         # Calculate attention scores\n#         attention_scores = torch.tanh(self.attention_fc(combined_features)).squeeze(-1)\n#         attention_weights = F.softmax(attention_scores, dim=0)\n        \n#         print('attn:',attention_weights.unsqueeze(0).size())\n#         # Apply attention weights to the ResNet features\n#         weighted_resnet_features = torch.bmm(attention_weights,resnet_output).squeeze(1)\n#         print('weight attn',weighted_resnet_features.size())\n\n#         # Concatenate attention-weighted ResNet features with BERT features\n#         fused_features = torch.cat((bert_output, weighted_resnet_features), dim=1)\n\n#         # Classification layer\n#         output = self.fc_layers(fused_features)\n#         output = F.softmax(output, dim=1)\n\n#         return output  ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# # # Define multimodal model\n# # class MultiModalModel(nn.Module):\n# #     def __init__(self, num_classes, bert_hidden_size=768):\n# #         super(MultiModalModel, self).__init__()  \n# #         # Text model (BERT)\n# #         self.bert = BertModel.from_pretrained('bert-base-uncased')\n# #         self.bert_out_features = self.bert.config.hidden_size\n        \n# #         # Image model (ResNet-152)\n# #         self.resnet = resnet152(pretrained=True)\n        \n# #         # Change the output layer to match the BERT output size\n# #         self.resnet.fc = nn.Linear(self.resnet.fc.in_features, self.bert_out_features)\n        \n# #         # Final classification layer\n# #         self.fc_layers = nn.Sequential(\n# #             nn.Linear(self.bert_out_features * 2, 128),  # Concatenating BERT and ResNet outputs\n# #             nn.LayerNorm(128),\n# #             nn.ReLU(),\n# #             nn.Linear(128, 64),\n# #             nn.LayerNorm(64),\n# #             nn.ReLU(),\n# #             nn.Linear(64, num_classes),\n# #         )\n\n# #     def forward(self, input_ids, attention_mask, image):\n# #         bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)[1]  # Taking pooled output\n# #         resnet_output = self.resnet(image)\n        \n# #         # Concatenate BERT and ResNet outputs\n# #         combined_features = torch.cat((bert_output, resnet_output), dim=1)\n        \n# #         # Classification layer\n# #         output = self.fc_layers(combined_features)\n# #         output = F.softmax(output, dim=1)\n# #         return output\n\n\n\n# import torch\n# import torch.nn as nn\n# import torch.nn.functional as F\n# from transformers import BertModel\n# from torchvision.models import vgg16\n\n# # Define multimodal model\n# class MultiModalModel(nn.Module):\n#     def __init__(self, num_classes, bert_hidden_size=768):\n#         super(MultiModalModel, self).__init__()  \n#         # Text model (BERT)\n#         self.bert = BertModel.from_pretrained('bert-base-uncased')\n#         self.bert_out_features = self.bert.config.hidden_size\n        \n#         # Image model (VGG-16)\n#         self.vgg16 = vgg16(pretrained=True)\n#         # Replace the final layer to match the BERT output size\n#         self.vgg16.classifier[-1] = nn.Linear(self.vgg16.classifier[-1].in_features, self.bert_out_features)\n        \n#         # Final classification layer\n#         self.fc_layers = nn.Sequential(\n#             nn.Linear(self.bert_out_features * 2, 128),  # Concatenating BERT and VGG outputs\n#             nn.LayerNorm(128),\n#             nn.ReLU(),\n#             nn.Linear(128, 64),\n#             nn.LayerNorm(64),\n#             nn.ReLU(),\n#             nn.Linear(64, num_classes),\n#         )\n\n#     def forward(self, input_ids, attention_mask, image):\n#         bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)[1]  # Taking pooled output\n#         vgg_output = self.vgg16(image)\n        \n#         # Concatenate BERT and VGG outputs\n#         combined_features = torch.cat((bert_output, vgg_output), dim=1)\n        \n#         # Classification layer\n#         output = self.fc_layers(combined_features)\n#         output = F.softmax(output, dim=1)\n#         return output\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import BertModel\nfrom torchvision.models import vgg16\n\nclass MultiModalModel(nn.Module):\n    def __init__(self, num_classes, bert_hidden_size=768, vgg_output_size=1000):\n        super(MultiModalModel, self).__init__()  \n        # Text model (BERT)\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.bert_out_features = self.bert.config.hidden_size\n        \n        # Image model (VGG-16)\n        self.vgg16 = vgg16(pretrained=True)\n        self.vgg_out_features = vgg_output_size\n        \n        # Attention mechanisms\n        self.text_att_linear = nn.Linear(self.bert_out_features, self.vgg_out_features)\n        self.image_att_linear = nn.Linear(self.vgg_out_features, self.bert_out_features)\n        \n        # Intermediate layers to adjust sizes\n        self.text_adjust = nn.Linear(self.vgg_out_features, self.bert_out_features)\n        self.image_adjust = nn.Linear(self.bert_out_features, self.vgg_out_features)\n        \n        # Final classification layer\n        self.fc_layers = nn.Sequential(\n            nn.Linear(self.bert_out_features + self.vgg_out_features, 128),  # Adjust input size\n            nn.LayerNorm(128),\n            nn.ReLU(),\n            nn.Linear(128, 64),\n            nn.LayerNorm(64),\n            nn.ReLU(),\n            nn.Linear(64, num_classes),\n        )\n\n    def forward(self, input_ids, attention_mask, image):\n        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask)[1]  # Taking pooled output\n        vgg_output = self.vgg16(image)\n        \n        # Attention mechanisms\n        text_att_weights = F.softmax(self.text_att_linear(bert_output), dim=1)\n        image_att_weights = F.softmax(self.image_att_linear(vgg_output), dim=1)\n        \n        # Adjust sizes of attention weights\n        text_att_weights_adjusted = self.text_adjust(text_att_weights)\n        image_att_weights_adjusted = self.image_adjust(image_att_weights)\n        \n        # Multiply original features with attention weights\n        attended_text_features = torch.mul(bert_output, text_att_weights_adjusted)\n        attended_image_features = torch.mul(vgg_output, image_att_weights_adjusted)\n        \n        # Concatenate attended features\n        combined_features = torch.cat((attended_text_features, attended_image_features), dim=1)\n        \n        # Classification layer\n        output = self.fc_layers(combined_features)\n        output = F.softmax(output, dim=1)\n        return output\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Training function\ndef train(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n    train_losses = []\n    val_losses = []\n    train_accs = []\n    val_accs = []\n\n    for epoch in range(num_epochs):\n        model.train()\n        running_loss = 0.0\n        correct = 0\n        total = 0\n        train_loader_iter = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n        for batch in train_loader_iter:\n            inputs, attention, images, labels = batch['input_ids'], batch['attention_mask'], batch['image'], batch['label']\n            optimizer.zero_grad()\n            outputs = model(inputs, attention, images)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == torch.argmax(labels, 1)).sum().item()\n            train_loader_iter.set_postfix({'loss': running_loss / total})\n        train_loss = running_loss / len(train_loader)\n        train_accuracy = correct / total\n        train_losses.append(train_loss)\n        train_accs.append(train_accuracy)\n\n        # Validation\n        model.eval()\n        val_loss = 0.0\n        correct = 0\n        total = 0\n        val_loader_iter = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False)\n        with torch.no_grad():\n            for batch in val_loader_iter:\n                inputs, attention, images, labels = batch['input_ids'], batch['attention_mask'], batch['image'], batch['label']\n                outputs = model(inputs, attention, images)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n                _, predicted = torch.max(outputs, 1)\n                total += labels.size(0)\n                correct += (predicted == torch.argmax(labels, 1)).sum().item()\n                val_loader_iter.set_postfix({'loss': loss.item()})\n        val_losses.append(val_loss / len(val_loader))\n        val_accuracy = correct / total\n        val_accs.append(val_accuracy)\n\n        print(f\"Epoch {epoch+1}/{num_epochs}, \"\n              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.4f}, \"\n              f\"Val Loss: {val_losses[-1]:.4f}, Val Acc: {val_accuracy:.4f}\")\n\n    return train_losses, val_losses, train_accs, val_accs\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n\n# Initialize transforms\ntransform = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.Resize((224, 224)),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Data preparation\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(device)\nIMAGE_PATH = NEW_ROOT_PATH + '/'\n# os.path.join(NEW_ROOT_PATH, \"/\")\ntrainSet = MemotionDataset(train_df,IMAGE_PATH, tokenizer, transform, device,mode = \"train\")\nvalSet = MemotionDataset(val_df,IMAGE_PATH, tokenizer, transform, device, mode = \"val\")\ntestSet = MemotionDataset(test_df,IMAGE_PATH, tokenizer, transform, device, mode = \"test\")\n\n# Hyperparameters\nnum_classes = 3\nbatch_size = 16\nlearning_rate = 0.001\nnum_epochs = 10\n\n# Initialize datasets and loaders\ntrain_loader = DataLoader(trainSet, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(valSet, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(testSet, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# os.listdir('/kaggle/working/data_images/')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize model\nmodel = MultiModalModel(num_classes).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n# model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_loader))\ninputs, attention, images, labels = batch['input_ids'], batch['attention_mask'], batch['image'], batch['label']","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train the model\ntrain_losses, val_losses, train_accs, val_accs = train(model, train_loader, val_loader, criterion, optimizer, num_epochs=num_epochs)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Plotting training and validation loss\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, num_epochs+1), train_losses, label='Train Loss')\nplt.plot(range(1, num_epochs+1), val_losses, label='Val Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\nplt.show()\n\n# Plotting training and validation accuracy\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, num_epochs+1), train_accs, label='Train Acc')\nplt.plot(range(1, num_epochs+1), val_accs, label='Val Acc')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Training and Validation Accuracy')\nplt.show()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import json\nimport os\nimport torch\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom transformers import AutoModel, AutoTokenizer, get_scheduler\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import AdamW, SGD\nfrom tqdm.notebook import tqdm, trange\nfrom time import perf_counter\nfrom PIL import Image\nimport pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# device = 'cpu'\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install wget","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nALBEF_FOLDER = 'ALBEF/'\nos.makedirs(ALBEF_FOLDER, exist_ok=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# download pre-trained ALBEF model and required ALBEF files from ALBEF's official repo (only need to do this once to save it in your gdrive)\n!wget https://raw.githubusercontent.com/salesforce/ALBEF/main/models/vit.py -O $ALBEF_FOLDER/vit.py\n!wget https://raw.githubusercontent.com/salesforce/ALBEF/main/models/tokenization_bert.py -O $ALBEF_FOLDER/tokenization_bert.py\n!wget https://raw.githubusercontent.com/salesforce/ALBEF/main/models/xbert.py -O $ALBEF_FOLDER/xbert.py\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# replace all occurrences of tokenizer_class with processor_class in xbert.py to make it compatible with newer transformers version\n# if you don't do this step, you will need to install transformers==4.8.1 as specified by the requirements in the ALBEF repo\n\n!sed -i 's/tokenizer_class/processor_class/g' $ALBEF_FOLDER/xbert.py","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# add path to downloaded ALBEF files\nimport sys\nsys.path.append(ALBEF_FOLDER)\n\n#import libraries required for ALBEF\nfrom vit import VisionTransformer\nfrom xbert import BertConfig as AlbefBertConfig, BertModel as AlbefBertModel\nfrom functools import partial\nimport numpy as np","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.dropna(axis=0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\ndef load_all_images_and_text(df, path):\n    images_paths = df['image_name'].to_list()\n    text = df['text'].to_list()\n    texts = []\n    label = df['label'].to_list()\n    labels = []\n    images = []\n    for ind, filename in enumerate(images_paths):\n        try:\n            img = Image.open(os.path.join(path,filename)).convert('RGB')\n        except:\n            continue\n        images.append(img)\n        texts.append(text[ind])\n        labels.append(label[ind])\n\n    return texts, images, labels\n\ntext, images, labels = load_all_images_and_text(data, path = NEW_ROOT_PATH)\n\nwith open(\"text\", \"wb\") as fp:\n    pickle.dump(text, fp)\n\nwith open(\"images\", \"wb\") as fp:\n    pickle.dump(images, fp)\n\nwith open(\"labels\", \"wb\") as fp:\n    pickle.dump(labels, fp)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(images)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AlbefDataset(Dataset):\n    def __init__(self, text, images, label):\n        self.text = text\n        self.images = images\n        self.label = label\n\n        # ALBEF settings\n        self.img_size = 256\n        self.mean, self.std = (\n            0.40821073), (0.26862954)\n\n\n        self.train_transform_func = transforms.Compose(\n                [transforms.RandomResizedCrop(self.img_size, scale=(0.5, 1.0)),\n                    transforms.RandomHorizontalFlip(),\n                    transforms.ToTensor(),\n                    transforms.Normalize(self.mean, self.std)\n                    ])\n\n                    \n    def __getitem__(self, idx):\n        text = self.text[idx]\n        label = self.label[idx]\n        if label == 'not_offensive':\n            label = 0\n        elif label == 'very_offensive':\n            label = 1\n        else:\n            label = 2\n        image = self.images[idx]\n        \n        \n        img = self.train_transform_func(image)\n\n        return text, img, label\n\n    def __len__(self):\n        return len(self.text)\n\ndataset = AlbefDataset(text, images, labels)\ndataset[2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AlbefModel(nn.Module):\n\n    def __init__(self, bert_config, num_labels, text_pretrained='bert-base-uncased'):\n        super().__init__()\n\n        self.num_labels = num_labels\n        self.text_encoder = AlbefBertModel.from_pretrained(\n            text_pretrained, config=bert_config, add_pooling_layer=False)\n\n        self.visual_encoder = VisionTransformer(\n            img_size=256, patch_size=16, embed_dim=768, depth=12, num_heads=12,\n            mlp_ratio=4, qkv_bias=True, norm_layer=partial(nn.LayerNorm, eps=1e-6))\n\n        self.classifier = nn.Linear(\n            self.text_encoder.config.hidden_size, num_labels)\n        \n    \n    def forward(self, text, image):\n        image_embeds = self.visual_encoder(image)\n        image_atts = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image_embeds.device)\n        output = self.text_encoder(text.input_ids, attention_mask=text.attention_mask,\n                                   encoder_hidden_states=image_embeds, encoder_attention_mask=image_atts, return_dict=True\n                                   )\n        logits = self.classifier(output.last_hidden_state[:, 0, :])\n        return logits","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from urllib.request import urlretrieve\n\ndef load_albef_pretrained(num_out_labels):\n    tmp_directory = './tmp/albef'\n    os.makedirs(tmp_directory, exist_ok=True)\n\n    albef_bert_config_fp = os.path.join(tmp_directory, 'config_bert.json')\n    albef_model_fp = os.path.join(tmp_directory, 'ALBEF.pth')\n\n    if not os.path.exists(albef_bert_config_fp):\n        urlretrieve(\"https://raw.githubusercontent.com/salesforce/ALBEF/main/configs/config_bert.json\", albef_bert_config_fp)\n\n    if not os.path.exists(albef_model_fp):\n        urlretrieve(\"https://storage.googleapis.com/sfr-pcl-data-research/ALBEF/ALBEF_4M.pth\", albef_model_fp)\n\n    albef_bert_config = AlbefBertConfig.from_json_file(albef_bert_config_fp)\n    albef_model = AlbefModel(bert_config=albef_bert_config, num_labels=num_out_labels)\n\n    print(albef_model_fp)\n    albef_checkpoint = torch.load(albef_model_fp, map_location='cpu')\n    albef_state_dict = albef_checkpoint['model']\n\n    for key in list(albef_state_dict.keys()):\n        if 'bert' in key:\n            encoder_key = key.replace('bert.', '')\n            albef_state_dict[encoder_key] = albef_state_dict[key]\n            del albef_state_dict[key]\n\n    msg = albef_model.load_state_dict(albef_state_dict, strict=False)\n    print(\"ALBEF checkpoint loaded from \", albef_model_fp)\n    # print(msg)\n    return albef_model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"albef_model = load_albef_pretrained(num_out_labels=1)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"albef_model.parameters()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoImageProcessor\nimport numpy as np\n\nb_inputs = bert_tokenizer(\n            \"not hello\", truncation=True, max_length=512,\n            return_tensors=\"pt\", padding='max_length'\n        ).to(device)\n\n# b_inputs = b_inputs.to(device)\nb_imgs = Image.open(NEW_ROOT_PATH + \"/image_445.jpg\")\nb_imgs = b_imgs.resize((256,256))\nb_imgs = torch.Tensor(np.array(b_imgs)).unsqueeze(0).permute(0, 3, 1, 2).to(device)\nb_inputs, b_imgs.shape\nb_imgs = b_imgs.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# x = albef_model(b_inputs, b_imgs)\n# x","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import time\n\n# # Start timer\n# start_time = time.perf_counter()\n# x = albef_model(b_inputs, b_imgs)\n# x.mean().backward()\n\n# end_time = time.perf_counter()\n\n# # Calculate elapsed time\n# elapsed_time = end_time - start_time\n# print(\"Elapsed time: \", elapsed_time)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data_loader = torch.utils.data.DataLoader(dataset, batch_size = 16, shuffle = True)\ntrain_data_loader","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchvision.ops import sigmoid_focal_loss\nnum_train_epochs = 5\nlearning_rate = 0.0005\nt_total = len(train_data_loader) * num_train_epochs\n\n\noptimizer = SGD(albef_model.parameters(), lr=learning_rate)\n# scheduler = get_scheduler(name=\"cosine\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=t_total)\n\ncriterion = sigmoid_focal_loss","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.cuda.empty_cache()\nalbef_model = albef_model.to(device)\nalbef_model.train()\n\nfor epoch_num in trange(num_train_epochs, desc='Epochs'):\n    epoch_total_loss = 0\n\n    for step, batch in tqdm(enumerate(train_data_loader), total=len(train_data_loader), desc='Batch'):        \n        b_text, b_imgs, b_labels, = batch  \n        # print(b_text, b_imgs, b_labels)\n        # break\n        b_inputs = bert_tokenizer(\n            list(b_text), truncation=True, max_length=128,\n            return_tensors=\"pt\", padding='max_length'\n        )\n        \n        b_labels = b_labels.to(device)\n        b_imgs = b_imgs.to(device)\n        b_inputs = b_inputs.to(device)\n\n        albef_model.zero_grad()        \n        b_logits = albef_model(text=b_inputs, image=b_imgs).squeeze(1)\n        \n        loss = criterion(b_logits.to(torch.float32), b_labels.to(torch.float32), reduction = 'mean')\n\n        epoch_total_loss += loss.item()\n\n        # Perform a backward pass to calculate the gradients\n        loss.backward()\n\n\n        optimizer.step()\n        # scheduler.step()\n        \n    avg_loss = epoch_total_loss/len(train_data_loader)\n\n\n    print('epoch =', epoch_num)\n    print('    epoch_loss =', epoch_total_loss)\n    print('    avg_epoch_loss =', avg_loss)\n    print('    learning rate =', optimizer.param_groups[0][\"lr\"])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"b_text","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}